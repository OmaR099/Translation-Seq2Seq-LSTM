{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyNDWZm/VVlOVzDLstX6JBLO"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","source":["#**Libraries**"],"metadata":{"id":"6DbSwHYNI0gz"}},{"cell_type":"code","execution_count":45,"metadata":{"id":"muYj6QpgIbUj","executionInfo":{"status":"ok","timestamp":1726682922851,"user_tz":-180,"elapsed":291,"user":{"displayName":"Omar Osama Mahmoud Mahmoud Hassanin","userId":"00892071859491103216"}}},"outputs":[],"source":["import string\n","import re\n","import numpy as np\n","import pandas as pd\n","import matplotlib.pyplot as plt\n","\n","import nltk\n","from nltk.tokenize import word_tokenize, sent_tokenize\n","\n","\n","import keras\n","from keras.models import Sequential\n","from keras.layers import Dense, LSTM, Embedding, RepeatVector\n","from tensorflow.keras.preprocessing.text import Tokenizer\n","from keras.callbacks import ModelCheckpoint\n","from keras.preprocessing.sequence import pad_sequences\n","from keras.models import load_model\n","from keras import optimizers"]},{"cell_type":"markdown","source":["# **Loading the Data and Configuration**\n"],"metadata":{"id":"TWA2fHE9I7bL"}},{"cell_type":"code","source":["# Read the text file\n","with open('ara.txt', 'r', encoding='utf-8') as file:\n","    data = file.read()  # Read the entire file content"],"metadata":{"id":"jX_4a3YZJDDT","executionInfo":{"status":"ok","timestamp":1726679946657,"user_tz":-180,"elapsed":305,"user":{"displayName":"Omar Osama Mahmoud Mahmoud Hassanin","userId":"00892071859491103216"}}},"execution_count":2,"outputs":[]},{"cell_type":"code","source":["batch_size = 64  # Batch size for training.\n","epochs = 10  # Number of epochs to train for.\n","latent_dim = 256  # Latent dimensionality of the encoding space.\n","num_samples = 10000  # Number of samples to train on."],"metadata":{"id":"0ExUKvFqQfID","executionInfo":{"status":"ok","timestamp":1726681848740,"user_tz":-180,"elapsed":301,"user":{"displayName":"Omar Osama Mahmoud Mahmoud Hassanin","userId":"00892071859491103216"}}},"execution_count":30,"outputs":[]},{"cell_type":"markdown","source":["# **Text Cleaning**"],"metadata":{"id":"yUASRcmIJSc6"}},{"cell_type":"code","source":["# Remove punctuation\n","translator = str.maketrans('', '', string.punctuation)\n","data = data.translate(translator)\n","\n","# Convert the text to lowercase\n","data = data.lower()  # Convert all characters to lowercase\n","\n","# Define a function to remove diacritics (tashkeel)\n","def remove_diacritics(text):\n","    # Diacritics pattern: this regex matches common Arabic diacritics\n","    diacritics_pattern = r'[\\u064B-\\u0652\\u0654-\\u0655]'\n","    # Substitute diacritics with an empty string\n","    return re.sub(diacritics_pattern, '', text)\n","\n","# Clean the Arabic text\n","data = remove_diacritics(data)\n","\n","# Save the lowercase text to a new file (optional)\n","with open('cleaned_text.txt', 'w', encoding='utf-8') as lowercase_file:\n","    lowercase_file.write(data)"],"metadata":{"id":"uaT_nt3bJR_D","executionInfo":{"status":"ok","timestamp":1726680758265,"user_tz":-180,"elapsed":284,"user":{"displayName":"Omar Osama Mahmoud Mahmoud Hassanin","userId":"00892071859491103216"}}},"execution_count":12,"outputs":[]},{"cell_type":"code","source":["# Printing first 15 lines in data file\n","num_lines = 15\n","\n","# Read the text file\n","with open('cleaned_text.txt', 'r', encoding='utf-8') as file:\n","    # Using list comprehension to read the first `num_lines`\n","    lines = [next(file) for _ in range(num_lines)]\n","\n","# Step 2: Print the lines\n","for line in lines:\n","    print(line, end='')  # The `end=''` prevents adding extra new lines"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"xH9tv6LRJ8Lj","executionInfo":{"status":"ok","timestamp":1726680761874,"user_tz":-180,"elapsed":313,"user":{"displayName":"Omar Osama Mahmoud Mahmoud Hassanin","userId":"00892071859491103216"}},"outputId":"fdefd9a7-6371-4824-be3a-07bb1f624b3d"},"execution_count":13,"outputs":[{"output_type":"stream","name":"stdout","text":["hi\tمرحبا\tccby 20 france attribution tatoebaorg 538123 cm  629296 samer\n","run\tاركض\tccby 20 france attribution tatoebaorg 906328 papabear  1245450 saeb\n","duck\tاخفض رأسك\tccby 20 france attribution tatoebaorg 280158 cm  9036391 keeichi\n","duck\tاخفضي رأسك\tccby 20 france attribution tatoebaorg 280158 cm  9036392 keeichi\n","duck\tاخفضوا رؤوسكم\tccby 20 france attribution tatoebaorg 280158 cm  9036393 keeichi\n","help\tالنجدة\tccby 20 france attribution tatoebaorg 435084 lukaszpp  371293 saeb\n","jump\tاقفز\tccby 20 france attribution tatoebaorg 1102981 jamessilver  6009426 damascene\n","stop\tقف\tccby 20 france attribution tatoebaorg 448320 cm  1245447 saeb\n","stop\tتوقف \tccby 20 france attribution tatoebaorg 448320 cm  5496702 wildflower81\n","wait\tإنتظر\tccby 20 france attribution tatoebaorg 1744314 belgavox  5496709 wildflower81\n","go on\tداوم\tccby 20 france attribution tatoebaorg 2230774 ck  5118652 damascene\n","go on\tاستمر\tccby 20 france attribution tatoebaorg 2230774 ck  5118653 damascene\n","hello\tمرحبا\tccby 20 france attribution tatoebaorg 373330 ck  1904219 asma\n","hello\tأهلا\tccby 20 france attribution tatoebaorg 1858850 languageexpert  1904218 asma\n","hello\tمرحبا\tccby 20 france attribution tatoebaorg 1858850 languageexpert  1904219 asma\n"]}]},{"cell_type":"markdown","source":["# **Prepare the data**\n"],"metadata":{"id":"jpKWtt-oMbxj"}},{"cell_type":"code","source":["# Data Initialization / Vectorize the data.\n","input_texts = []\n","target_texts = []\n","input_characters = set()\n","target_characters = set()"],"metadata":{"id":"yv8FfbJ3RrGE","executionInfo":{"status":"ok","timestamp":1726682625843,"user_tz":-180,"elapsed":311,"user":{"displayName":"Omar Osama Mahmoud Mahmoud Hassanin","userId":"00892071859491103216"}}},"execution_count":33,"outputs":[]},{"cell_type":"code","source":["# Read Cleaned and Process the File\n","with open('cleaned_text.txt', \"r\", encoding=\"utf-8\") as f:\n","    lines = f.read().split(\"\\n\")"],"metadata":{"id":"82ESv9LCR9KI","executionInfo":{"status":"ok","timestamp":1726682627392,"user_tz":-180,"elapsed":329,"user":{"displayName":"Omar Osama Mahmoud Mahmoud Hassanin","userId":"00892071859491103216"}}},"execution_count":34,"outputs":[]},{"cell_type":"code","source":["# Populate Input and Target Texts\n","for line in lines[: min(num_samples, len(lines) - 1)]:\n","    input_text, target_text, _ = line.split(\"\\t\")\n","    # We use \"tab\" as the \"start sequence\" character\n","    # for the targets, and \"\\n\" as \"end sequence\" character.\n","    target_text = \"\\t\" + target_text + \"\\n\" # start of the sequence (SOS) is /t and EOS /n\n","    input_texts.append(input_text)\n","    target_texts.append(target_text)\n","    for char in input_text:\n","        if char not in input_characters:\n","            input_characters.add(char)\n","    for char in target_text:\n","        if char not in target_characters:\n","            target_characters.add(char)"],"metadata":{"id":"AacepTbJSDgb","executionInfo":{"status":"ok","timestamp":1726682628970,"user_tz":-180,"elapsed":327,"user":{"displayName":"Omar Osama Mahmoud Mahmoud Hassanin","userId":"00892071859491103216"}}},"execution_count":35,"outputs":[]},{"cell_type":"code","source":["# Character Information\n","input_characters = sorted(list(input_characters))\n","target_characters = sorted(list(target_characters))\n","num_encoder_tokens = len(input_characters) # Store the number of unique tokens in the input texts.\n","num_decoder_tokens = len(target_characters) # Store the number of unique tokens in the target texts.\n","max_encoder_seq_length = max([len(txt) for txt in input_texts])\n","max_decoder_seq_length = max([len(txt) for txt in target_texts])"],"metadata":{"id":"hcZSd5U3SmfC","executionInfo":{"status":"ok","timestamp":1726682630471,"user_tz":-180,"elapsed":316,"user":{"displayName":"Omar Osama Mahmoud Mahmoud Hassanin","userId":"00892071859491103216"}}},"execution_count":36,"outputs":[]},{"cell_type":"code","source":["# Summary of Information\n","print(\"Number of samples:\", len(input_texts))\n","print(\"Number of unique input tokens:\", num_encoder_tokens)\n","print(\"Number of unique output tokens:\", num_decoder_tokens)\n","print(\"Max sequence length for inputs:\", max_encoder_seq_length)\n","print(\"Max sequence length for outputs:\", max_decoder_seq_length)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"su6Zk0ZyTKxC","executionInfo":{"status":"ok","timestamp":1726682631998,"user_tz":-180,"elapsed":359,"user":{"displayName":"Omar Osama Mahmoud Mahmoud Hassanin","userId":"00892071859491103216"}},"outputId":"c8155644-ef76-495c-f66f-93ce6f4c285e"},"execution_count":37,"outputs":[{"output_type":"stream","name":"stdout","text":["Number of samples: 10000\n","Number of unique input tokens: 38\n","Number of unique output tokens: 74\n","Max sequence length for inputs: 35\n","Max sequence length for outputs: 53\n"]}]},{"cell_type":"code","source":["# Character Index Maps\n","input_token_index = dict([(char, i) for i, char in enumerate(input_characters)])\n","target_token_index = dict([(char, i) for i, char in enumerate(target_characters)])"],"metadata":{"id":"ohVGWcPeTBpz","executionInfo":{"status":"ok","timestamp":1726682636194,"user_tz":-180,"elapsed":301,"user":{"displayName":"Omar Osama Mahmoud Mahmoud Hassanin","userId":"00892071859491103216"}}},"execution_count":38,"outputs":[]},{"cell_type":"code","source":["# Initialize Sequence Arrays\n","encoder_input_data = np.zeros(\n","    (len(input_texts), max_encoder_seq_length, num_encoder_tokens),\n","    dtype=\"float32\",\n",")\n","decoder_input_data = np.zeros(\n","    (len(input_texts), max_decoder_seq_length, num_decoder_tokens),\n","    dtype=\"float32\",\n",")\n","decoder_target_data = np.zeros(\n","    (len(input_texts), max_decoder_seq_length, num_decoder_tokens),\n","    dtype=\"float32\",\n",")"],"metadata":{"id":"WTyaOkqTTX7i","executionInfo":{"status":"ok","timestamp":1726682639602,"user_tz":-180,"elapsed":319,"user":{"displayName":"Omar Osama Mahmoud Mahmoud Hassanin","userId":"00892071859491103216"}}},"execution_count":39,"outputs":[]},{"cell_type":"code","source":["# Populate the Input and Output Arrays\n","for i, (input_text, target_text) in enumerate(zip(input_texts, target_texts)):\n","    for t, char in enumerate(input_text):\n","        encoder_input_data[i, t, input_token_index[char]] = 1.0\n","    encoder_input_data[i, t + 1 :, input_token_index[\" \"]] = 1.0\n","    for t, char in enumerate(target_text):\n","        # decoder_target_data is ahead of decoder_input_data by one timestep\n","        decoder_input_data[i, t, target_token_index[char]] = 1.0\n","        if t > 0:\n","            # decoder_target_data will be ahead by one timestep\n","            # and will not include the start character.\n","            decoder_target_data[i, t - 1, target_token_index[char]] = 1.0\n","    decoder_input_data[i, t + 1 :, target_token_index[\" \"]] = 1.0\n","    decoder_target_data[i, t:, target_token_index[\" \"]] = 1.0"],"metadata":{"id":"-Iu4q3OcTbGz","executionInfo":{"status":"ok","timestamp":1726682643924,"user_tz":-180,"elapsed":911,"user":{"displayName":"Omar Osama Mahmoud Mahmoud Hassanin","userId":"00892071859491103216"}}},"execution_count":40,"outputs":[]},{"cell_type":"markdown","source":["# **Model Buliding and Training**"],"metadata":{"id":"H7dzET84UD5r"}},{"cell_type":"code","source":["# Define an input sequence and process it.\n","encoder_inputs = keras.Input(shape=(None, num_encoder_tokens))\n","encoder = keras.layers.LSTM(latent_dim, return_state=True)\n","encoder_outputs, state_h, state_c = encoder(encoder_inputs)\n","\n","# We discard `encoder_outputs` and only keep the states.\n","encoder_states = [state_h, state_c]\n","\n","# Set up the decoder, using `encoder_states` as initial state.\n","decoder_inputs = keras.Input(shape=(None, num_decoder_tokens))\n","\n","# We set up our decoder to return full output sequences,\n","# and to return internal states as well. We don't use the\n","# return states in the training model, but we will use them in inference.\n","decoder_lstm = keras.layers.LSTM(latent_dim, return_sequences=True, return_state=True)\n","decoder_outputs, _, _ = decoder_lstm(decoder_inputs, initial_state=encoder_states)\n","decoder_dense = keras.layers.Dense(num_decoder_tokens, activation=\"softmax\")\n","decoder_outputs = decoder_dense(decoder_outputs)\n","\n","# Define the model that will turn\n","# `encoder_input_data` & `decoder_input_data` into `decoder_target_data`\n","model = keras.Model([encoder_inputs, decoder_inputs], decoder_outputs)"],"metadata":{"id":"27LNb9RkULJi","executionInfo":{"status":"ok","timestamp":1726682925980,"user_tz":-180,"elapsed":446,"user":{"displayName":"Omar Osama Mahmoud Mahmoud Hassanin","userId":"00892071859491103216"}}},"execution_count":46,"outputs":[]},{"cell_type":"code","source":["model.summary()"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":386},"id":"3OBSyTmEYIhz","executionInfo":{"status":"ok","timestamp":1726683849926,"user_tz":-180,"elapsed":321,"user":{"displayName":"Omar Osama Mahmoud Mahmoud Hassanin","userId":"00892071859491103216"}},"outputId":"267e3561-34f6-4b6d-c43e-e2f5fca4507d"},"execution_count":48,"outputs":[{"output_type":"display_data","data":{"text/plain":["\u001b[1mModel: \"functional\"\u001b[0m\n"],"text/html":["<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"functional\"</span>\n","</pre>\n"]},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["┏━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┓\n","┃\u001b[1m \u001b[0m\u001b[1mLayer (type)             \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape          \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m       Param #\u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mConnected to          \u001b[0m\u001b[1m \u001b[0m┃\n","┡━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━┩\n","│ input_layer (\u001b[38;5;33mInputLayer\u001b[0m)  │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m38\u001b[0m)       │              \u001b[38;5;34m0\u001b[0m │ -                      │\n","├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n","│ input_layer_1             │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m74\u001b[0m)       │              \u001b[38;5;34m0\u001b[0m │ -                      │\n","│ (\u001b[38;5;33mInputLayer\u001b[0m)              │                        │                │                        │\n","├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n","│ lstm (\u001b[38;5;33mLSTM\u001b[0m)               │ [(\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m256\u001b[0m), (\u001b[38;5;45mNone\u001b[0m,   │        \u001b[38;5;34m302,080\u001b[0m │ input_layer[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]      │\n","│                           │ \u001b[38;5;34m256\u001b[0m), (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m256\u001b[0m)]     │                │                        │\n","├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n","│ lstm_1 (\u001b[38;5;33mLSTM\u001b[0m)             │ [(\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m256\u001b[0m),    │        \u001b[38;5;34m338,944\u001b[0m │ input_layer_1[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m],   │\n","│                           │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m256\u001b[0m), (\u001b[38;5;45mNone\u001b[0m,    │                │ lstm[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m1\u001b[0m], lstm[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m2\u001b[0m] │\n","│                           │ \u001b[38;5;34m256\u001b[0m)]                  │                │                        │\n","├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n","│ dense (\u001b[38;5;33mDense\u001b[0m)             │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m74\u001b[0m)       │         \u001b[38;5;34m19,018\u001b[0m │ lstm_1[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]           │\n","└───────────────────────────┴────────────────────────┴────────────────┴────────────────────────┘\n"],"text/html":["<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┓\n","┃<span style=\"font-weight: bold\"> Layer (type)              </span>┃<span style=\"font-weight: bold\"> Output Shape           </span>┃<span style=\"font-weight: bold\">        Param # </span>┃<span style=\"font-weight: bold\"> Connected to           </span>┃\n","┡━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━┩\n","│ input_layer (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">InputLayer</span>)  │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">38</span>)       │              <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ -                      │\n","├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n","│ input_layer_1             │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">74</span>)       │              <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ -                      │\n","│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">InputLayer</span>)              │                        │                │                        │\n","├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n","│ lstm (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">LSTM</span>)               │ [(<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>), (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>,   │        <span style=\"color: #00af00; text-decoration-color: #00af00\">302,080</span> │ input_layer[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]      │\n","│                           │ <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>), (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)]     │                │                        │\n","├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n","│ lstm_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">LSTM</span>)             │ [(<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>),    │        <span style=\"color: #00af00; text-decoration-color: #00af00\">338,944</span> │ input_layer_1[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>],   │\n","│                           │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>), (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>,    │                │ lstm[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">1</span>], lstm[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">2</span>] │\n","│                           │ <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)]                  │                │                        │\n","├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n","│ dense (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)             │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">74</span>)       │         <span style=\"color: #00af00; text-decoration-color: #00af00\">19,018</span> │ lstm_1[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]           │\n","└───────────────────────────┴────────────────────────┴────────────────┴────────────────────────┘\n","</pre>\n"]},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["\u001b[1m Total params: \u001b[0m\u001b[38;5;34m1,320,086\u001b[0m (5.04 MB)\n"],"text/html":["<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">1,320,086</span> (5.04 MB)\n","</pre>\n"]},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m660,042\u001b[0m (2.52 MB)\n"],"text/html":["<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">660,042</span> (2.52 MB)\n","</pre>\n"]},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"],"text/html":["<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n","</pre>\n"]},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["\u001b[1m Optimizer params: \u001b[0m\u001b[38;5;34m660,044\u001b[0m (2.52 MB)\n"],"text/html":["<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Optimizer params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">660,044</span> (2.52 MB)\n","</pre>\n"]},"metadata":{}}]},{"cell_type":"code","source":["model.compile(\n","    optimizer=\"rmsprop\", loss=\"categorical_crossentropy\", metrics=[\"accuracy\"]\n",")\n","model.fit(\n","    [encoder_input_data, decoder_input_data],\n","    decoder_target_data,\n","    batch_size=batch_size,\n","    epochs=epochs,\n","    validation_split=0.2,\n",")\n","# Save model\n","model.save(\"seq2seq_arabic2english_model.keras\")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"asTWFDVyVO-T","executionInfo":{"status":"ok","timestamp":1726683789637,"user_tz":-180,"elapsed":658017,"user":{"displayName":"Omar Osama Mahmoud Mahmoud Hassanin","userId":"00892071859491103216"}},"outputId":"7b9a606f-dc98-4146-b08b-06bb9026f4fe"},"execution_count":47,"outputs":[{"output_type":"stream","name":"stdout","text":["Epoch 1/10\n","\u001b[1m125/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m59s\u001b[0m 453ms/step - accuracy: 0.6898 - loss: 1.5874 - val_accuracy: 0.6009 - val_loss: 1.6918\n","Epoch 2/10\n","\u001b[1m125/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m72s\u001b[0m 580ms/step - accuracy: 0.7249 - loss: 1.0829 - val_accuracy: 0.6007 - val_loss: 1.5348\n","Epoch 3/10\n","\u001b[1m125/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m58s\u001b[0m 466ms/step - accuracy: 0.7296 - loss: 1.0130 - val_accuracy: 0.5997 - val_loss: 1.4514\n","Epoch 4/10\n","\u001b[1m125/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m56s\u001b[0m 449ms/step - accuracy: 0.7453 - loss: 0.9413 - val_accuracy: 0.6287 - val_loss: 1.3569\n","Epoch 5/10\n","\u001b[1m125/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m81s\u001b[0m 446ms/step - accuracy: 0.7577 - loss: 0.9071 - val_accuracy: 0.6543 - val_loss: 1.2825\n","Epoch 6/10\n","\u001b[1m125/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m82s\u001b[0m 447ms/step - accuracy: 0.7644 - loss: 0.8665 - val_accuracy: 0.6553 - val_loss: 1.2690\n","Epoch 7/10\n","\u001b[1m125/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m56s\u001b[0m 447ms/step - accuracy: 0.7710 - loss: 0.8385 - val_accuracy: 0.6641 - val_loss: 1.2272\n","Epoch 8/10\n","\u001b[1m125/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m56s\u001b[0m 445ms/step - accuracy: 0.7749 - loss: 0.8209 - val_accuracy: 0.6689 - val_loss: 1.2082\n","Epoch 9/10\n","\u001b[1m125/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m82s\u001b[0m 445ms/step - accuracy: 0.7770 - loss: 0.8079 - val_accuracy: 0.6701 - val_loss: 1.1930\n","Epoch 10/10\n","\u001b[1m125/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m54s\u001b[0m 433ms/step - accuracy: 0.7817 - loss: 0.7881 - val_accuracy: 0.6710 - val_loss: 1.1852\n"]}]},{"cell_type":"markdown","source":["# **Run inference (Sampling)**\n"],"metadata":{"id":"aD_pMObMVvjy"}},{"cell_type":"code","source":["# Define sampling models\n","# Restore the model and construct the encoder and decoder.\n","model = keras.models.load_model(\"seq2seq_arabic2english_model.keras\")\n","\n","encoder_inputs = model.input[0]  # input_1\n","encoder_outputs, state_h_enc, state_c_enc = model.layers[2].output  # lstm_1\n","encoder_states = [state_h_enc, state_c_enc]\n","encoder_model = keras.Model(encoder_inputs, encoder_states)\n","\n","decoder_inputs = model.input[1]  # input_2\n","decoder_state_input_h = keras.Input(shape=(latent_dim,))\n","decoder_state_input_c = keras.Input(shape=(latent_dim,))\n","decoder_states_inputs = [decoder_state_input_h, decoder_state_input_c]\n","decoder_lstm = model.layers[3]\n","decoder_outputs, state_h_dec, state_c_dec = decoder_lstm(\n","    decoder_inputs, initial_state=decoder_states_inputs\n",")\n","decoder_states = [state_h_dec, state_c_dec]\n","decoder_dense = model.layers[4]\n","decoder_outputs = decoder_dense(decoder_outputs)\n","decoder_model = keras.Model(\n","    [decoder_inputs] + decoder_states_inputs, [decoder_outputs] + decoder_states\n",")\n","\n","# Reverse-lookup token index to decode sequences back to\n","# something readable.\n","reverse_input_char_index = dict((i, char) for char, i in input_token_index.items())\n","reverse_target_char_index = dict((i, char) for char, i in target_token_index.items())\n","\n","\n","def decode_sequence(input_seq):\n","    # Encode the input as state vectors.\n","    states_value = encoder_model.predict(input_seq, verbose=0)\n","\n","    # Generate empty target sequence of length 1.\n","    target_seq = np.zeros((1, 1, num_decoder_tokens))\n","    # Populate the first character of target sequence with the start character.\n","    target_seq[0, 0, target_token_index[\"\\t\"]] = 1.0\n","\n","    # Sampling loop for a batch of sequences\n","    # (to simplify, here we assume a batch of size 1).\n","    stop_condition = False\n","    decoded_sentence = \"\"\n","    while not stop_condition:\n","        output_tokens, h, c = decoder_model.predict(\n","            [target_seq] + states_value, verbose=0\n","        )\n","\n","        # Sample a token\n","        sampled_token_index = np.argmax(output_tokens[0, -1, :])\n","        sampled_char = reverse_target_char_index[sampled_token_index]\n","        decoded_sentence += sampled_char\n","\n","        # Exit condition: either hit max length\n","        # or find stop character.\n","        if sampled_char == \"\\n\" or len(decoded_sentence) > max_decoder_seq_length:\n","            stop_condition = True\n","\n","        # Update the target sequence (of length 1).\n","        target_seq = np.zeros((1, 1, num_decoder_tokens))\n","        target_seq[0, 0, sampled_token_index] = 1.0\n","\n","        # Update states\n","        states_value = [h, c]\n","    return decoded_sentence"],"metadata":{"id":"thLxygE5VzwT","executionInfo":{"status":"ok","timestamp":1726683867092,"user_tz":-180,"elapsed":301,"user":{"displayName":"Omar Osama Mahmoud Mahmoud Hassanin","userId":"00892071859491103216"}}},"execution_count":49,"outputs":[]},{"cell_type":"code","source":["for seq_index in range(20):\n","    # Take one sequence (part of the training set)\n","    # for trying out decoding.\n","    input_seq = encoder_input_data[seq_index : seq_index + 1]\n","    decoded_sentence = decode_sequence(input_seq)\n","    print(\"-\")\n","    print(\"Input sentence:\", input_texts[seq_index])\n","    print(\"Decoded sentence:\", decoded_sentence)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"NWamzwf4V6ND","executionInfo":{"status":"ok","timestamp":1726683909914,"user_tz":-180,"elapsed":39195,"user":{"displayName":"Omar Osama Mahmoud Mahmoud Hassanin","userId":"00892071859491103216"}},"outputId":"6cb9215f-6838-4025-87c3-5214965326d1"},"execution_count":50,"outputs":[{"output_type":"stream","name":"stdout","text":["-\n","Input sentence: hi\n","Decoded sentence: أنا توم المارة\n","\n","-\n","Input sentence: run\n","Decoded sentence: أنا توم المارة\n","\n","-\n","Input sentence: duck\n","Decoded sentence: أنا توم المارة\n","\n","-\n","Input sentence: duck\n","Decoded sentence: أنا توم المارة\n","\n","-\n","Input sentence: duck\n","Decoded sentence: أنا توم المارة\n","\n","-\n","Input sentence: help\n","Decoded sentence: أنا أن أن أن أن أن المار\n","\n","-\n","Input sentence: jump\n","Decoded sentence: أنا توم المارة\n","\n","-\n","Input sentence: stop\n","Decoded sentence: أنا توم المارة\n","\n","-\n","Input sentence: stop\n","Decoded sentence: أنا توم المارة\n","\n","-\n","Input sentence: wait\n","Decoded sentence: أنا توم المارة\n","\n","-\n","Input sentence: go on\n","Decoded sentence: أنا توم المارة\n","\n","-\n","Input sentence: go on\n","Decoded sentence: أنا توم المارة\n","\n","-\n","Input sentence: hello\n","Decoded sentence: أنا أن أن أن أن أن المار\n","\n","-\n","Input sentence: hello\n","Decoded sentence: أنا أن أن أن أن أن المار\n","\n","-\n","Input sentence: hello\n","Decoded sentence: أنا أن أن أن أن أن المار\n","\n","-\n","Input sentence: hurry\n","Decoded sentence: أنا أن أن أن أن أن المار\n","\n","-\n","Input sentence: hurry\n","Decoded sentence: أنا أن أن أن أن أن المار\n","\n","-\n","Input sentence: i see\n","Decoded sentence: أنا توم المارة\n","\n","-\n","Input sentence: i won\n","Decoded sentence: أنا توم المارة\n","\n","-\n","Input sentence: relax\n","Decoded sentence: أنا أن أن أن أن أن المار\n","\n"]}]}]}